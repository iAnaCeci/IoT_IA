{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WI60d2i8Hhw",
        "outputId": "eb68a6b2-be37-458d-d7c7-e3bfd32db843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 39.8MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 1.07MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 10.0MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.18MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HARD] Epoch 01 | Loss 0.2519 | Acc 97.90%\n",
            "[HARD] Epoch 02 | Loss 0.0605 | Acc 98.52%\n",
            "[HARD] Epoch 03 | Loss 0.0428 | Acc 98.59%\n",
            "[HARD] Epoch 04 | Loss 0.0320 | Acc 98.88%\n",
            "[HARD] Epoch 05 | Loss 0.0234 | Acc 98.80%\n",
            "[SMOOTH] Epoch 01 | Loss 0.6972 | Acc 98.64%\n",
            "[SMOOTH] Epoch 02 | Loss 0.5656 | Acc 99.01%\n",
            "[SMOOTH] Epoch 03 | Loss 0.5479 | Acc 99.03%\n",
            "[SMOOTH] Epoch 04 | Loss 0.5389 | Acc 99.18%\n",
            "[SMOOTH] Epoch 05 | Loss 0.5328 | Acc 99.26%\n",
            "[MIXUP] Epoch 01 | Loss 1.0801 | Acc 98.32%\n",
            "[MIXUP] Epoch 02 | Loss 0.9498 | Acc 98.60%\n",
            "[MIXUP] Epoch 03 | Loss 0.9204 | Acc 98.75%\n",
            "[MIXUP] Epoch 04 | Loss 0.9005 | Acc 98.94%\n",
            "[MIXUP] Epoch 05 | Loss 0.8644 | Acc 98.98%\n",
            "\n",
            "ðŸ“Š Resultados finais (AcurÃ¡cia no MNIST):\n",
            "    hard: 98.80%\n",
            "  smooth: 99.26%\n",
            "   mixup: 98.98%\n",
            "Salvos: mnist_hard_acc.png, mnist_soft_acc.png, mnist_mixup_acc.png, mnist_hard_loss.png, mnist_soft_loss.png, mnist_mixup_loss.png, mnist_acc_comparison.png, mnist_loss_comparison.png\n"
          ]
        }
      ],
      "source": [
        "# pip install torch torchvision torchaudio -q\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# ------------------------------\n",
        "# CNN simples para MNIST\n",
        "# ------------------------------\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): return self.fc(self.conv(x))\n",
        "\n",
        "# ------------------------------\n",
        "# FunÃ§Ãµes auxiliares\n",
        "# ------------------------------\n",
        "def soft_cross_entropy(logits, soft_targets):\n",
        "    log_probs = F.log_softmax(logits, dim=1)\n",
        "    return -(soft_targets * log_probs).sum(dim=1).mean()\n",
        "\n",
        "def label_smoothing(one_hot, smoothing=0.1):\n",
        "    return one_hot * (1 - smoothing) + smoothing / one_hot.size(1)\n",
        "\n",
        "def to_one_hot(y, num_classes=NUM_CLASSES):\n",
        "    return F.one_hot(y, num_classes=num_classes).float()\n",
        "\n",
        "def mixup(x, y_onehot, alpha=0.4):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    index = torch.randperm(x.size(0), device=x.device)\n",
        "    x_mix = lam * x + (1 - lam) * x[index]\n",
        "    y_mix = lam * y_onehot + (1 - lam) * y_onehot[index]\n",
        "    return x_mix, y_mix\n",
        "\n",
        "# ------------------------------\n",
        "# Dataset MNIST\n",
        "# ------------------------------\n",
        "tfm = transforms.Compose([transforms.ToTensor()])\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
        "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=tfm)\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256)\n",
        "\n",
        "# ------------------------------\n",
        "# FunÃ§Ã£o de treino\n",
        "# ------------------------------\n",
        "def train_model(mode=\"hard\", epochs=5):\n",
        "    model = CNN().to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        loss_total = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_onehot = to_one_hot(y)\n",
        "\n",
        "            # Hard labels (padrÃ£o)\n",
        "            if mode == \"hard\":\n",
        "                y_soft = y_onehot\n",
        "                loss_fn = nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(model(x), y)\n",
        "\n",
        "            #  Soft labels (Label Smoothing)\n",
        "            elif mode == \"smooth\":\n",
        "                y_soft = label_smoothing(y_onehot, 0.1)\n",
        "                loss = soft_cross_entropy(model(x), y_soft)\n",
        "\n",
        "            # Soft labels + MixUp\n",
        "            elif mode == \"mixup\":\n",
        "                y_soft = label_smoothing(y_onehot, 0.1)\n",
        "                x, y_soft = mixup(x, y_soft, 0.4)\n",
        "                loss = soft_cross_entropy(model(x), y_soft)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss_total += loss.item() * x.size(0)\n",
        "\n",
        "        # AvaliaÃ§Ã£o\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.inference_mode():\n",
        "            for x, y in test_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                preds = model(x).argmax(dim=1)\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.size(0)\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"[{mode.upper()}] Epoch {epoch:02d} | Loss {loss_total/len(train_loader.dataset):.4f} | Acc {acc:.2f}%\")\n",
        "    return model, acc\n",
        "\n",
        "# ------------------------------\n",
        "# Treinar e comparar\n",
        "# ------------------------------\n",
        "modes = [\"hard\", \"smooth\", \"mixup\"]\n",
        "results = {}\n",
        "for m in modes:\n",
        "    model, acc = train_model(m, epochs=5)\n",
        "    results[m] = acc\n",
        "\n",
        "print(\"\\n Resultados finais (AcurÃ¡cia no MNIST):\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k:>8s}: {v:.2f}%\")\n",
        "    # -*- coding: utf-8 -*-\n",
        "# Gera grÃ¡ficos de MNIST: hard vs soft (smoothing e mixup)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = [1, 2, 3, 4, 5]\n",
        "\n",
        "hard_loss   = [0.2154, 0.0558, 0.0386, 0.0295, 0.0224]\n",
        "hard_acc    = [98.13, 98.77, 98.60, 98.83, 98.98]\n",
        "\n",
        "smooth_loss = [0.7168, 0.5681, 0.5502, 0.5408, 0.5343]\n",
        "smooth_acc  = [98.15, 98.85, 99.08, 99.01, 99.27]\n",
        "\n",
        "mixup_loss  = [1.1016, 0.9423, 0.9165, 0.9013, 0.8695]\n",
        "mixup_acc   = [97.99, 98.39, 98.57, 98.66, 98.98]\n",
        "\n",
        "def save_line(xs, ys, title, ylabel, outpath):\n",
        "    plt.figure()\n",
        "    plt.plot(xs, ys, marker='o')                 # (nÃ£o define cores)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outpath, dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "# Um grÃ¡fico por mÃ©todo â€” ACC\n",
        "save_line(epochs, hard_acc,   'MNIST â€” Accuracy per Epoch (Hard labels)', 'Accuracy (%)', 'mnist_hard_acc.png')\n",
        "save_line(epochs, smooth_acc, 'MNIST â€” Accuracy per Epoch (Soft: Label Smoothing)', 'Accuracy (%)', 'mnist_soft_acc.png')\n",
        "save_line(epochs, mixup_acc,  'MNIST â€” Accuracy per Epoch (Soft: MixUp)', 'Accuracy (%)', 'mnist_mixup_acc.png')\n",
        "\n",
        "# Um grÃ¡fico por mÃ©todo â€” LOSS\n",
        "save_line(epochs, hard_loss,   'MNIST â€” Loss per Epoch (Hard labels)', 'Loss', 'mnist_hard_loss.png')\n",
        "save_line(epochs, smooth_loss, 'MNIST â€” Loss per Epoch (Soft: Label Smoothing)', 'Loss', 'mnist_soft_loss.png')\n",
        "save_line(epochs, mixup_loss,  'MNIST â€” Loss per Epoch (Soft: MixUp)', 'Loss', 'mnist_mixup_loss.png')\n",
        "\n",
        "# Comparativos (opcionais) â€” ACC\n",
        "plt.figure()\n",
        "plt.plot(epochs, hard_acc,   marker='o', label='Hard')\n",
        "plt.plot(epochs, smooth_acc, marker='o', label='Soft (Smoothing)')\n",
        "plt.plot(epochs, mixup_acc,  marker='o', label='Soft (MixUp)')\n",
        "plt.title('MNIST â€” Accuracy per Epoch (Comparison)')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.legend()\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout(); plt.savefig('mnist_acc_comparison.png', dpi=160); plt.close()\n",
        "\n",
        "# Comparativos (opcionais) â€” LOSS\n",
        "plt.figure()\n",
        "plt.plot(epochs, hard_loss,   marker='o', label='Hard')\n",
        "plt.plot(epochs, smooth_loss, marker='o', label='Soft (Smoothing)')\n",
        "plt.plot(epochs, mixup_loss,  marker='o', label='Soft (MixUp)')\n",
        "plt.title('MNIST â€” Loss per Epoch (Comparison)')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
        "plt.grid(True, linestyle='--', linewidth=0.5)\n",
        "plt.tight_layout(); plt.savefig('mnist_loss_comparison.png', dpi=160); plt.close()\n",
        "\n",
        "print(\"Salvos: mnist_hard_acc.png, mnist_soft_acc.png, mnist_mixup_acc.png,\",\n",
        "      \"mnist_hard_loss.png, mnist_soft_loss.png, mnist_mixup_loss.png,\",\n",
        "      \"mnist_acc_comparison.png, mnist_loss_comparison.png\")\n",
        "\n"
      ]
    }
  ]
}